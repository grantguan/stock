# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171CnWt85dtwCuQb-hK58q7DQjAUKStHa
"""

import numpy as np
import pandas as pd

CLASSES = ["Bull", "Bear"]
LABEL_BULL = CLASSES.index("Bull")
LABEL_BEAR = CLASSES.index("Bear")

datasets = np.load("datasets.npz")
x_train, y_train = datasets["x_train"], datasets["y_train"]
x_val, y_val = datasets["x_val"], datasets["y_val"]
x_test, y_test = datasets["x_test"], datasets["y_test"]

label_distribution = pd.DataFrame([{
    "Dataset": "train",
    "Bull": np.count_nonzero(y_train == LABEL_BULL),
    "Bear": np.count_nonzero(y_train == LABEL_BEAR)},{
    "Dataset": "val",
    "Bull": np.count_nonzero(y_val == LABEL_BULL),
    "Bear": np.count_nonzero(y_val == LABEL_BEAR)},{
    "Dataset": "test",
    "Bull": np.count_nonzero(y_test == LABEL_BULL),
    "Bear": np.count_nonzero(y_test == LABEL_BEAR)},
])
print(label_distribution)
"""# ***Construct model***"""

# !pip install tensorflow
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

def inception_module(input_tensor):
    bottleneck = Conv1D(filters=32, kernel_size=1, padding="same", activation=None, use_bias=False)(input_tensor)
    conv3 = Conv1D(filters=32, kernel_size=3, padding="same", activation=None, use_bias=False)(bottleneck)
    conv5 = Conv1D(filters=32, kernel_size=5, padding="same", activation=None, use_bias=False)(bottleneck)
    conv7 = Conv1D(filters=32, kernel_size=7, padding="same", activation=None, use_bias=False)(bottleneck)
    mp = MaxPool1D(pool_size=3, strides=1, padding="same")(input_tensor)
    mpbottleneck = Conv1D(filters=32, kernel_size=1, padding="same", activation=None, use_bias=False)(mp)
    x = Concatenate(axis=-1)([conv3, conv5, conv7, mpbottleneck])
    x = BatchNormalization()(x)
    x = Activation("LeakyReLU")(x)
    return x

def shortcut_layer(input_tensor1, input_tensor2):
    shortcut = Conv1D(filters=input_tensor2.shape[-1], kernel_size=1, padding="same", activation=None, use_bias=False)(input_tensor1)
    shortcut = BatchNormalization()(shortcut)
    x = Add()([shortcut, input_tensor2])
    x = Activation("LeakyReLU")(x)
    return x

import tensorflow as tf

# 设置可见的GPU设备
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # 只使用第一个GPU设备
        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
        # 设置内存增长选项
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError as e:
        print(e)

n_time_steps = x_train.shape[1]
n_features = x_train.shape[2]

input_layer = Input(shape=(n_time_steps, n_features))
x = input_layer
input_residual = input_layer

for i in range(6):
    x = inception_module(x)
    x = Dropout(0.2)(x)

    if i % 3 == 2:
        x = shortcut_layer(input_residual, x)
        input_residual = x
x = GlobalAveragePooling1D()(x)
output_layer = Dense(len(CLASSES), activation="softmax")(x)

model = Model(inputs=input_layer, outputs=output_layer)
model.summary()

"""# ***Train model***"""

from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import to_categorical

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

train_label_counts = label_distribution.iloc[0]
class_weight = {
    LABEL_BULL: train_label_counts["Bear"] / train_label_counts["Bull"],
    LABEL_BEAR: 1.
}
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.2, patience=30, min_lr= 0.00001)
model_checkpoint = ModelCheckpoint(filepath="best_model.hdf5", monitor="val_loss", save_best_only=True)
early_stopping = EarlyStopping(monitor="val_loss", patience=100, restore_best_weights=True)
callbacks = [reduce_lr, model_checkpoint, early_stopping]

train_history = model.fit(x_train, to_categorical(y_train), 
                          class_weight=class_weight,
                          validation_data=(x_val, to_categorical(y_val)),
                          batch_size=2048, epochs=1000, callbacks=[callbacks])

import matplotlib.pyplot as plt


plt.style.use("seaborn-v0_8")
fig, axes = plt.subplots(2, 1, figsize=(16, 12))

axes[0].set_title("Loss")
axes[0].set_yscale("log")
axes[0].plot(train_history.history["loss"], label="Training")
axes[0].plot(train_history.history["val_loss"], label="Validation")
axes[0].legend()

axes[1].set_title("Accuracy")
axes[1].set_xlabel("Epoch")
axes[1].plot(train_history.history["accuracy"], label="Training")
axes[1].plot(train_history.history["val_accuracy"], label="Validation")
axes[1].legend()